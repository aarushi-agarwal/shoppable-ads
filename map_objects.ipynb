{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aarushia/Library/Python/3.13/lib/python/site-packages/open_clip/factory.py:380: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 352x640 2 persons, 36.7ms\n",
      "Speed: 10.9ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Results saved to matched_results.csv\n",
      "   detected_object_index                                 best_match_product  \\\n",
      "0                      0  Linen with Blouse Piece Saree (GSKI14456_Green...   \n",
      "1                      1                                    Cotton Jumpsuit   \n",
      "\n",
      "   similarity_score  \n",
      "0          0.295117  \n",
      "1          0.240274  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load YOLOv8 model for object detection\n",
    "yolo_model = YOLO(\"yolov8n.pt\")  # Use a pre-trained YOLO model\n",
    "\n",
    "# Load CLIP model for embedding extraction\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model_name = \"ViT-B-32\"\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(clip_model_name, pretrained=\"openai\", device=device)\n",
    "clip_tokenizer = open_clip.get_tokenizer(clip_model_name)\n",
    "\n",
    "# Load stored embeddings from file\n",
    "stored_embeddings_df = pd.read_csv(\"clip_text_embeddings.csv\")\n",
    "\n",
    "# Convert stored embeddings from string to numpy array\n",
    "stored_embeddings_df[\"embedding\"] = stored_embeddings_df[\"embedding\"].apply(lambda x: np.array(eval(x)))\n",
    "stored_product_titles = stored_embeddings_df[\"product_title\"].tolist()\n",
    "stored_embeddings = np.vstack(stored_embeddings_df[\"embedding\"].values)\n",
    "\n",
    "# Function to process image and extract objects\n",
    "def detect_objects(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    results = yolo_model(image)  # Run object detection\n",
    "\n",
    "    detected_objects = []\n",
    "    for result in results:\n",
    "        for box in result.boxes.xyxy:  # Get bounding boxes\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cropped_img = image[y1:y2, x1:x2]  # Crop detected object\n",
    "            detected_objects.append(cropped_img)\n",
    "    \n",
    "    return detected_objects\n",
    "\n",
    "# Function to compute CLIP embeddings for detected objects\n",
    "def get_clip_embedding(image):\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    image_tensor = clip_preprocess(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embedding = clip_model.encode_image(image_tensor)\n",
    "\n",
    "    image_embedding /= image_embedding.norm(dim=-1, keepdim=True)  # Normalize\n",
    "    return image_embedding.cpu().numpy()\n",
    "\n",
    "# Function to match detected objects to stored embeddings\n",
    "def match_objects(image_path):\n",
    "    detected_objects = detect_objects(image_path)\n",
    "    results = []\n",
    "\n",
    "    for obj in detected_objects:\n",
    "        obj_embedding = get_clip_embedding(obj)\n",
    "        similarities = cosine_similarity(obj_embedding, stored_embeddings)  # Compare with stored embeddings\n",
    "        best_match_idx = np.argmax(similarities)  # Find closest match\n",
    "        best_match_title = stored_product_titles[best_match_idx]\n",
    "        best_match_score = similarities[0, best_match_idx]\n",
    "\n",
    "        results.append({\n",
    "            \"detected_object_index\": len(results),\n",
    "            \"best_match_product\": best_match_title,\n",
    "            \"similarity_score\": best_match_score\n",
    "        })\n",
    "\n",
    "    # Save results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    #df_results.to_csv(\"matched_results.csv\", index=False)\n",
    "    print(\"Results saved to matched_results.csv\")\n",
    "    print(df_results)\n",
    "\n",
    "\n",
    "# Run the matching\n",
    "image_path = \"images/test_image.png\"  # Change this to your image path\n",
    "match_objects(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 5 persons, 1 couch, 1 tv, 72.6ms\n",
      "Speed: 7.9ms preprocess, 72.6ms inference, 11.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to matched_results.csv\n",
      "   detected_object_index                                 best_match_product  \\\n",
      "0                      0  Women's Satin Silk Printed Square Scarf Dupatt...   \n",
      "1                      1  Men's Tweed Woollen Bandhgala Waistcoat Brown ...   \n",
      "2                      2  Women's Satin Silk Printed Square Scarf Dupatt...   \n",
      "3                      3                      mens Extras size (s14_indo_p)   \n",
      "4                      4  Boys Sherwani Set Full Sleeve Nehru Collared G...   \n",
      "5                      5  Men Gold-Coloured &amp; Copper-Toned Self Desi...   \n",
      "6                      6                     Indo pants For Men (UP-RI-516)   \n",
      "\n",
      "   similarity_score  \n",
      "0          0.383973  \n",
      "1          0.402881  \n",
      "2          0.414169  \n",
      "3          0.113357  \n",
      "4          0.311615  \n",
      "5          0.116813  \n",
      "6          0.088393  \n"
     ]
    }
   ],
   "source": [
    "# map objects using fine-tuned clip \n",
    "\n",
    "import torch\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ðŸ”¹ Load YOLOv8 model for object detection\n",
    "yolo_model = YOLO(\"yolov8n.pt\")  # Use a pre-trained YOLO model\n",
    "\n",
    "# ðŸ”¹ Load Fine-Tuned CLIP Model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model_name = \"ViT-B-32\"\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(clip_model_name, pretrained=\"openai\", device=device)\n",
    "\n",
    "# ðŸ”¹ Load fine-tuned weights\n",
    "fine_tuned_weights = \"finetuned_clip.pth\"\n",
    "clip_model.load_state_dict(torch.load(fine_tuned_weights, map_location=device))  # Load fine-tuned CLIP\n",
    "clip_model.eval()  # Set model to evaluation mode\n",
    "clip_tokenizer = open_clip.get_tokenizer(clip_model_name)\n",
    "\n",
    "# ðŸ”¹ Load stored embeddings from file (generated using the fine-tuned model)\n",
    "stored_embeddings_df = pd.read_csv(\"clip_text_embeddings.csv\")\n",
    "\n",
    "# Convert stored embeddings from string to numpy array\n",
    "stored_embeddings_df[\"embedding\"] = stored_embeddings_df[\"embedding\"].apply(lambda x: np.array(eval(x)))\n",
    "stored_product_titles = stored_embeddings_df[\"product_title\"].tolist()\n",
    "stored_embeddings = np.vstack(stored_embeddings_df[\"embedding\"].values)\n",
    "\n",
    "# ðŸ”¹ Function to process image and extract objects\n",
    "def detect_objects(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    results = yolo_model(image)  # Run object detection\n",
    "\n",
    "    detected_objects = []\n",
    "    for result in results:\n",
    "        for box in result.boxes.xyxy:  # Get bounding boxes\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cropped_img = image[y1:y2, x1:x2]  # Crop detected object\n",
    "            detected_objects.append(cropped_img)\n",
    "    \n",
    "    return detected_objects\n",
    "\n",
    "# ðŸ”¹ Function to compute CLIP embeddings for detected objects (using fine-tuned model)\n",
    "def get_clip_embedding(image):\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    image_tensor = clip_preprocess(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embedding = clip_model.encode_image(image_tensor)  # Use fine-tuned model\n",
    "\n",
    "    image_embedding /= image_embedding.norm(dim=-1, keepdim=True)  # Normalize\n",
    "    return image_embedding.cpu().numpy()\n",
    "\n",
    "# ðŸ”¹ Function to match detected objects to stored embeddings\n",
    "def match_objects(image_path):\n",
    "    detected_objects = detect_objects(image_path)\n",
    "    results = []\n",
    "\n",
    "    for obj in detected_objects:\n",
    "        obj_embedding = get_clip_embedding(obj)\n",
    "        similarities = cosine_similarity(obj_embedding, stored_embeddings)  # Compare with fine-tuned embeddings\n",
    "        best_match_idx = np.argmax(similarities)  # Find closest match\n",
    "        best_match_title = stored_product_titles[best_match_idx]\n",
    "        best_match_score = similarities[0, best_match_idx]\n",
    "\n",
    "        results.append({\n",
    "            \"detected_object_index\": len(results),\n",
    "            \"best_match_product\": best_match_title,\n",
    "            \"similarity_score\": best_match_score\n",
    "        })\n",
    "\n",
    "    # ðŸ”¹ Save results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(\"matched_results.csv\", index=False)\n",
    "    print(\"Results saved to matched_results.csv\")\n",
    "    print(df_results)\n",
    "\n",
    "# ðŸ”¹ Run the matching\n",
    "image_path = \"ksbkbt_s1e1_frames/keyframe_165.jpg\"  # Change this to your image path\n",
    "match_objects(image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
