{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aarushia/Library/Python/3.13/lib/python/site-packages/open_clip/factory.py:380: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 5 potted plants, 71.1ms\n",
      "Speed: 5.9ms preprocess, 71.1ms inference, 10.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 chair, 1 couch, 1 potted plant, 41.2ms\n",
      "Speed: 1.7ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 30.9ms\n",
      "Speed: 1.5ms preprocess, 30.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 chairs, 1 couch, 1 bed, 40.5ms\n",
      "Speed: 1.1ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bv/4tdgz9vj5k7fsth3zs_6l3dh0000gq/T/ipykernel_77308/4097905058.py:101: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_results = pd.concat([df_results, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 34.8ms\n",
      "Speed: 1.4ms preprocess, 34.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 couch, 1 bed, 29.9ms\n",
      "Speed: 1.3ms preprocess, 29.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 40.2ms\n",
      "Speed: 1.1ms preprocess, 40.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 couch, 1 bed, 1 dining table, 24.5ms\n",
      "Speed: 8.3ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 31.6ms\n",
      "Speed: 1.1ms preprocess, 31.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 couch, 1 bed, 1 dining table, 24.3ms\n",
      "Speed: 1.7ms preprocess, 24.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 51.8ms\n",
      "Speed: 1.1ms preprocess, 51.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 125.4ms\n",
      "Speed: 1.2ms preprocess, 125.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 33.1ms\n",
      "Speed: 1.5ms preprocess, 33.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.4ms\n",
      "Speed: 2.2ms preprocess, 28.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 3 books, 81.9ms\n",
      "Speed: 15.3ms preprocess, 81.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 34.3ms\n",
      "Speed: 5.9ms preprocess, 34.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 33.6ms\n",
      "Speed: 1.2ms preprocess, 33.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.3ms\n",
      "Speed: 1.4ms preprocess, 27.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 chairs, 1 couch, 1 potted plant, 1 dining table, 2 books, 25.4ms\n",
      "Speed: 1.1ms preprocess, 25.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 30.6ms\n",
      "Speed: 1.5ms preprocess, 30.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 2 chairs, 1 couch, 1 potted plant, 1 dining table, 2 books, 29.3ms\n",
      "Speed: 1.2ms preprocess, 29.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 potted plant, 32.9ms\n",
      "Speed: 1.2ms preprocess, 32.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tie, 26.8ms\n",
      "Speed: 1.4ms preprocess, 26.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 29.9ms\n",
      "Speed: 1.7ms preprocess, 29.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.5ms\n",
      "Speed: 1.4ms preprocess, 27.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 chair, 36.5ms\n",
      "Speed: 1.4ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 bottle, 34.7ms\n",
      "Speed: 1.2ms preprocess, 34.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.6ms\n",
      "Speed: 1.3ms preprocess, 22.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.0ms\n",
      "Speed: 1.5ms preprocess, 26.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 chair, 26.4ms\n",
      "Speed: 1.2ms preprocess, 26.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 41.0ms\n",
      "Speed: 1.3ms preprocess, 41.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 34.1ms\n",
      "Speed: 1.1ms preprocess, 34.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.4ms\n",
      "Speed: 1.2ms preprocess, 28.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tie, 22.2ms\n",
      "Speed: 1.1ms preprocess, 22.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 1.4ms preprocess, 38.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.9ms\n",
      "Speed: 1.7ms preprocess, 28.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 ties, 22.1ms\n",
      "Speed: 1.3ms preprocess, 22.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.6ms\n",
      "Speed: 1.1ms preprocess, 25.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.7ms\n",
      "Speed: 1.2ms preprocess, 23.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 bottle, 1 chair, 23.6ms\n",
      "Speed: 1.6ms preprocess, 23.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 potted plant, 1 remote, 24.2ms\n",
      "Speed: 1.1ms preprocess, 24.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.4ms\n",
      "Speed: 1.1ms preprocess, 24.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.6ms\n",
      "Speed: 1.2ms preprocess, 22.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.8ms\n",
      "Speed: 1.8ms preprocess, 23.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 potted plant, 1 microwave, 24.4ms\n",
      "Speed: 1.4ms preprocess, 24.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 37.9ms\n",
      "Speed: 4.1ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.9ms\n",
      "Speed: 1.3ms preprocess, 23.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 22.8ms\n",
      "Speed: 1.3ms preprocess, 22.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 23.1ms\n",
      "Speed: 1.1ms preprocess, 23.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 24.3ms\n",
      "Speed: 1.1ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 22.0ms\n",
      "Speed: 1.2ms preprocess, 22.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 tie, 26.0ms\n",
      "Speed: 1.1ms preprocess, 26.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 23.5ms\n",
      "Speed: 1.1ms preprocess, 23.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 23.9ms\n",
      "Speed: 1.3ms preprocess, 23.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 chair, 1 couch, 1 tv, 25.1ms\n",
      "Speed: 1.1ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 30.1ms\n",
      "Speed: 1.5ms preprocess, 30.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 113.6ms\n",
      "Speed: 4.7ms preprocess, 113.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 30.6ms\n",
      "Speed: 1.5ms preprocess, 30.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.7ms\n",
      "Speed: 1.1ms preprocess, 24.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 27.3ms\n",
      "Speed: 1.2ms preprocess, 27.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.7ms\n",
      "Speed: 1.1ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 chairs, 26.4ms\n",
      "Speed: 1.1ms preprocess, 26.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 24.7ms\n",
      "Speed: 1.1ms preprocess, 24.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.8ms\n",
      "Speed: 3.5ms preprocess, 84.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 chair, 1 tv, 23.9ms\n",
      "Speed: 1.1ms preprocess, 23.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 tie, 175.1ms\n",
      "Speed: 3.4ms preprocess, 175.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 suitcase, 1 bottle, 22.7ms\n",
      "Speed: 1.1ms preprocess, 22.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 24.2ms\n",
      "Speed: 1.5ms preprocess, 24.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 potted plant, 29.9ms\n",
      "Speed: 1.2ms preprocess, 29.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.5ms\n",
      "Speed: 1.5ms preprocess, 25.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 61.1ms\n",
      "Speed: 1.4ms preprocess, 61.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.9ms\n",
      "Speed: 1.4ms preprocess, 23.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 26.0ms\n",
      "Speed: 1.2ms preprocess, 26.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 33.1ms\n",
      "Speed: 1.2ms preprocess, 33.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 couch, 1 tv, 27.6ms\n",
      "Speed: 1.1ms preprocess, 27.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 5.0ms preprocess, 34.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.6ms\n",
      "Speed: 1.2ms preprocess, 24.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.5ms\n",
      "Speed: 1.1ms preprocess, 24.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 123.6ms\n",
      "Speed: 1.2ms preprocess, 123.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 7.0ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.6ms\n",
      "Speed: 1.3ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 39.9ms\n",
      "Speed: 1.4ms preprocess, 39.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 32.9ms\n",
      "Speed: 1.9ms preprocess, 32.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 29.9ms\n",
      "Speed: 1.1ms preprocess, 29.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 22.7ms\n",
      "Speed: 1.7ms preprocess, 22.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 1.1ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 28.2ms\n",
      "Speed: 1.1ms preprocess, 28.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.2ms\n",
      "Speed: 1.1ms preprocess, 24.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 couch, 26.8ms\n",
      "Speed: 1.2ms preprocess, 26.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.4ms\n",
      "Speed: 1.1ms preprocess, 23.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 potted plant, 1 remote, 31.0ms\n",
      "Speed: 1.2ms preprocess, 31.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.1ms\n",
      "Speed: 1.1ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.6ms\n",
      "Speed: 1.2ms preprocess, 22.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 1 potted plant, 28.4ms\n",
      "Speed: 5.3ms preprocess, 28.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.0ms\n",
      "Speed: 1.2ms preprocess, 26.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 spoon, 2 bananas, 1 orange, 23.8ms\n",
      "Speed: 1.2ms preprocess, 23.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 1 potted plant, 24.0ms\n",
      "Speed: 1.1ms preprocess, 24.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 fork, 2 bananas, 1 orange, 37.5ms\n",
      "Speed: 1.9ms preprocess, 37.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 23.5ms\n",
      "Speed: 1.3ms preprocess, 23.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 2 bananas, 1 orange, 23.4ms\n",
      "Speed: 1.1ms preprocess, 23.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 potted plant, 22.4ms\n",
      "Speed: 1.1ms preprocess, 22.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 banana, 1 orange, 23.8ms\n",
      "Speed: 1.3ms preprocess, 23.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 potted plant, 24.5ms\n",
      "Speed: 1.2ms preprocess, 24.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 banana, 1 orange, 1 cell phone, 22.4ms\n",
      "Speed: 1.1ms preprocess, 22.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 3 cakes, 4 chairs, 1 potted plant, 1 dining table, 2 vases, 63.2ms\n",
      "Speed: 1.1ms preprocess, 63.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 1.5ms preprocess, 42.4ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 23.5ms\n",
      "Speed: 1.2ms preprocess, 23.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 potted plant, 1 vase, 24.6ms\n",
      "Speed: 1.5ms preprocess, 24.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 wine glass, 1 spoon, 1 banana, 1 orange, 22.8ms\n",
      "Speed: 1.2ms preprocess, 22.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.5ms\n",
      "Speed: 5.3ms preprocess, 26.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 wine glass, 1 banana, 1 orange, 29.0ms\n",
      "Speed: 1.5ms preprocess, 29.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 1 potted plant, 22.0ms\n",
      "Speed: 1.2ms preprocess, 22.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 119.3ms\n",
      "Speed: 11.4ms preprocess, 119.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 46.1ms\n",
      "Speed: 1.5ms preprocess, 46.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.5ms\n",
      "Speed: 1.2ms preprocess, 24.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 1 refrigerator, 23.6ms\n",
      "Speed: 1.2ms preprocess, 23.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.0ms\n",
      "Speed: 1.1ms preprocess, 22.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 wine glass, 5 chairs, 1 potted plant, 1 dining table, 2 vases, 58.4ms\n",
      "Speed: 10.6ms preprocess, 58.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 potted plant, 24.2ms\n",
      "Speed: 1.2ms preprocess, 24.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.4ms\n",
      "Speed: 1.1ms preprocess, 25.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 potted plant, 23.0ms\n",
      "Speed: 1.2ms preprocess, 23.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tie, 22.7ms\n",
      "Speed: 1.1ms preprocess, 22.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 potted plant, 23.3ms\n",
      "Speed: 1.1ms preprocess, 23.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 46.9ms\n",
      "Speed: 4.2ms preprocess, 46.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 29.6ms\n",
      "Speed: 1.1ms preprocess, 29.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 ties, 24.0ms\n",
      "Speed: 1.4ms preprocess, 24.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.6ms\n",
      "Speed: 1.1ms preprocess, 23.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.5ms\n",
      "Speed: 1.5ms preprocess, 24.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tie, 52.2ms\n",
      "Speed: 14.1ms preprocess, 52.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.5ms\n",
      "Speed: 1.1ms preprocess, 23.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 45.8ms\n",
      "Speed: 1.1ms preprocess, 45.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.4ms\n",
      "Speed: 1.1ms preprocess, 28.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 1.2ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.5ms\n",
      "Speed: 1.1ms preprocess, 25.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.3ms\n",
      "Speed: 1.1ms preprocess, 23.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.3ms\n",
      "Speed: 1.2ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.6ms\n",
      "Speed: 1.1ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 1.1ms preprocess, 35.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.6ms\n",
      "Speed: 1.2ms preprocess, 22.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.5ms\n",
      "Speed: 1.2ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 chairs, 24.2ms\n",
      "Speed: 1.4ms preprocess, 24.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.6ms\n",
      "Speed: 1.1ms preprocess, 22.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tie, 24.0ms\n",
      "Speed: 1.2ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 5 chairs, 23.3ms\n",
      "Speed: 1.1ms preprocess, 23.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 5 chairs, 23.7ms\n",
      "Speed: 1.5ms preprocess, 23.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 4 chairs, 24.0ms\n",
      "Speed: 1.5ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.1ms\n",
      "Speed: 1.1ms preprocess, 23.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.2ms\n",
      "Speed: 1.1ms preprocess, 22.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.9ms\n",
      "Speed: 1.1ms preprocess, 22.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.3ms\n",
      "Speed: 1.2ms preprocess, 25.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 22.7ms\n",
      "Speed: 1.2ms preprocess, 22.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 4 chairs, 50.3ms\n",
      "Speed: 1.2ms preprocess, 50.3ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 21.5ms\n",
      "Speed: 1.1ms preprocess, 21.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 4 chairs, 24.8ms\n",
      "Speed: 1.1ms preprocess, 24.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.7ms\n",
      "Speed: 1.2ms preprocess, 23.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.5ms\n",
      "Speed: 1.2ms preprocess, 27.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.6ms\n",
      "Speed: 1.1ms preprocess, 22.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.6ms\n",
      "Speed: 1.2ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 book, 22.5ms\n",
      "Speed: 1.4ms preprocess, 22.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.4ms\n",
      "Speed: 1.1ms preprocess, 23.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.6ms\n",
      "Speed: 1.2ms preprocess, 25.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.7ms\n",
      "Speed: 1.5ms preprocess, 23.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.6ms\n",
      "Speed: 12.3ms preprocess, 25.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.9ms\n",
      "Speed: 1.3ms preprocess, 22.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 29.6ms\n",
      "Speed: 1.6ms preprocess, 29.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 30.4ms\n",
      "Speed: 1.5ms preprocess, 30.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.6ms\n",
      "Speed: 1.8ms preprocess, 28.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 book, 25.9ms\n",
      "Speed: 1.3ms preprocess, 25.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.5ms\n",
      "Speed: 1.5ms preprocess, 26.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 1.5ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 clock, 24.6ms\n",
      "Speed: 1.4ms preprocess, 24.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 1.6ms preprocess, 38.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 30.9ms\n",
      "Speed: 1.5ms preprocess, 30.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.9ms\n",
      "Speed: 1.1ms preprocess, 24.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 30.3ms\n",
      "Speed: 1.2ms preprocess, 30.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.9ms\n",
      "Speed: 1.3ms preprocess, 23.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 34.4ms\n",
      "Speed: 1.3ms preprocess, 34.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.1ms\n",
      "Speed: 1.2ms preprocess, 82.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 potted plant, 82.5ms\n",
      "Speed: 8.6ms preprocess, 82.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.3ms\n",
      "Speed: 1.1ms preprocess, 26.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 couch, 24.2ms\n",
      "Speed: 1.2ms preprocess, 24.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 books, 24.1ms\n",
      "Speed: 1.1ms preprocess, 24.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "                Frame Person_ID  Best_Match   Score\n",
      "0     keyframe_10.jpg         1     A Saree  0.6333\n",
      "1    keyframe_100.jpg         1     A Kurta  0.9506\n",
      "2    keyframe_100.jpg         2     A Saree  0.4273\n",
      "3    keyframe_102.jpg         1     A Saree  0.7914\n",
      "4    keyframe_102.jpg         2  A Necklace  0.3983\n",
      "..                ...       ...         ...     ...\n",
      "331   keyframe_96.jpg         2     A Saree  0.7255\n",
      "332   keyframe_97.jpg         1     A Saree  0.7421\n",
      "333   keyframe_98.jpg         1     A Saree  0.8915\n",
      "334   keyframe_98.jpg         2     A Kurta  0.9350\n",
      "335   keyframe_99.jpg         1     A Kurta  0.9720\n",
      "\n",
      "[336 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "import certifi\n",
    "import torch\n",
    "import open_clip\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "# Fix SSL issues\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "ssl._create_default_https_context = ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "# Load YOLOv8 model\n",
    "yolo_model = YOLO(\"yolov8n.pt\")  # Use \"yolov8m.pt\" or \"yolov8l.pt\" for better accuracy\n",
    "\n",
    "# Load OpenCLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess, _ = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "# Define text descriptions for classification\n",
    "text_descriptions = [\n",
    "    \"A Saree\",\n",
    "    \"A woman wearing a saree\",\n",
    "    \"A traditional Indian saree with embroidery\",\n",
    "    \"A long dress\",\n",
    "    \"A person wearing a t-shirt\",\n",
    "    \"A person wearing pants\",\n",
    "    \"A Kurta\",\n",
    "    \"A t-shirt\",\n",
    "    \"A Necklace\"\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "# Tokenize text descriptions for CLIP\n",
    "text_inputs = open_clip.tokenize(text_descriptions).to(device)\n",
    "\n",
    "# Define the folder containing frames\n",
    "frame_folder = \"/Users/aarushia/Downloads/shows/frames/kyuki saas/ksbkbt_s1e1_thres30\"  # Update with the correct path\n",
    "\n",
    "# Ensure the frame folder exists\n",
    "if not os.path.exists(frame_folder):\n",
    "    raise FileNotFoundError(f\"Frame folder not found: {frame_folder}\")\n",
    "\n",
    "# Get all frame images\n",
    "frame_files = sorted([f for f in os.listdir(frame_folder) if f.endswith((\".jpg\", \".png\"))])\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "df_results = pd.DataFrame(columns=[\"Frame\", \"Person_ID\", \"Best_Match\", \"Score\"])\n",
    "\n",
    "# Loop through all frames\n",
    "for frame_id, frame_file in enumerate(frame_files):\n",
    "    frame_path = os.path.join(frame_folder, frame_file)\n",
    "    image = cv2.imread(frame_path)\n",
    "    if image is None:\n",
    "        continue  # Skip if the image cannot be loaded\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for PIL\n",
    "\n",
    "    # Run YOLOv8 detection on the frame\n",
    "    results = yolo_model(image)\n",
    "\n",
    "    # Process each detected object\n",
    "    person_count = 0\n",
    "    for result in results:\n",
    "        for box, cls in zip(result.boxes.xyxy, result.boxes.cls):\n",
    "            if int(cls) == 0:  # Class 0 in COCO dataset is \"Person\"\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # Crop lower body (remove face region)\n",
    "                height = y2 - y1\n",
    "                y1 += int(height * 0.3)  # Remove top 30% to focus on clothing\n",
    "\n",
    "                cropped_image = image_rgb[y1:y2, x1:x2]\n",
    "                if cropped_image.shape[0] == 0 or cropped_image.shape[1] == 0:\n",
    "                    continue  # Skip if cropping resulted in an empty image\n",
    "\n",
    "                cropped_pil = Image.fromarray(cropped_image)  # Convert to PIL format\n",
    "\n",
    "                # Preprocess cropped image for CLIP\n",
    "                image_tensor = preprocess(cropped_pil).unsqueeze(0).to(device)\n",
    "\n",
    "                # Compute CLIP image and text embeddings\n",
    "                with torch.no_grad():\n",
    "                    image_features = model.encode_image(image_tensor)\n",
    "                    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "                # Compute cosine similarity\n",
    "                similarity = (image_features @ text_features.T).softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "                # Get best matching label\n",
    "                best_match_idx = similarity.argmax()\n",
    "                best_match = text_descriptions[best_match_idx]\n",
    "                best_match_score = similarity[0][best_match_idx]\n",
    "\n",
    "                # Append to DataFrame\n",
    "                df_results = pd.concat([df_results, pd.DataFrame([{\n",
    "                    \"Frame\": frame_file,\n",
    "                    \"Person_ID\": person_count + 1,\n",
    "                    \"Best_Match\": best_match,\n",
    "                    \"Score\": round(best_match_score, 4)\n",
    "                }])], ignore_index=True)\n",
    "\n",
    "                person_count += 1  # Increment person count in the frame\n",
    "\n",
    "\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"results_2.csv\", index=False)  # Save results to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aarushia/Library/Python/3.13/lib/python/site-packages/open_clip/factory.py:380: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 5 potted plants, 48.5ms\n",
      "Speed: 2.1ms preprocess, 48.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bv/4tdgz9vj5k7fsth3zs_6l3dh0000gq/T/ipykernel_77308/696176069.py:112: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_results = pd.concat([df_results, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 chair, 1 couch, 1 potted plant, 26.3ms\n",
      "Speed: 1.1ms preprocess, 26.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.0ms\n",
      "Speed: 4.3ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 chairs, 1 couch, 1 bed, 24.0ms\n",
      "Speed: 1.1ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 44.3ms\n",
      "Speed: 1.6ms preprocess, 44.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 couch, 1 bed, 63.2ms\n",
      "Speed: 1.2ms preprocess, 63.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 40.4ms\n",
      "Speed: 1.3ms preprocess, 40.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 couch, 1 bed, 1 dining table, 79.0ms\n",
      "Speed: 2.3ms preprocess, 79.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 34.7ms\n",
      "Speed: 1.4ms preprocess, 34.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 couch, 1 bed, 1 dining table, 46.4ms\n",
      "Speed: 9.6ms preprocess, 46.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 26.0ms\n",
      "Speed: 1.1ms preprocess, 26.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.8ms\n",
      "Speed: 1.2ms preprocess, 23.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 32.5ms\n",
      "Speed: 1.2ms preprocess, 32.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.4ms\n",
      "Speed: 1.3ms preprocess, 25.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 3 books, 25.7ms\n",
      "Speed: 1.1ms preprocess, 25.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 26.4ms\n",
      "Speed: 1.4ms preprocess, 26.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.0ms\n",
      "Speed: 1.2ms preprocess, 23.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.4ms\n",
      "Speed: 1.1ms preprocess, 25.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 chairs, 1 couch, 1 potted plant, 1 dining table, 2 books, 25.5ms\n",
      "Speed: 1.2ms preprocess, 25.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.5ms\n",
      "Speed: 1.2ms preprocess, 24.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 2 chairs, 1 couch, 1 potted plant, 1 dining table, 2 books, 26.1ms\n",
      "Speed: 1.2ms preprocess, 26.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 potted plant, 34.9ms\n",
      "Speed: 1.2ms preprocess, 34.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tie, 25.1ms\n",
      "Speed: 1.2ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 35.1ms\n",
      "Speed: 1.1ms preprocess, 35.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.8ms\n",
      "Speed: 1.2ms preprocess, 24.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 chair, 24.9ms\n",
      "Speed: 1.2ms preprocess, 24.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 bottle, 24.7ms\n",
      "Speed: 1.1ms preprocess, 24.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 31.5ms\n",
      "Speed: 1.8ms preprocess, 31.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.6ms\n",
      "Speed: 1.7ms preprocess, 27.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 chair, 25.2ms\n",
      "Speed: 1.2ms preprocess, 25.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 34.1ms\n",
      "Speed: 1.2ms preprocess, 34.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.9ms\n",
      "Speed: 1.4ms preprocess, 26.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.3ms\n",
      "Speed: 1.2ms preprocess, 26.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tie, 23.8ms\n",
      "Speed: 1.1ms preprocess, 23.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.8ms\n",
      "Speed: 1.3ms preprocess, 24.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.4ms\n",
      "Speed: 1.8ms preprocess, 28.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 ties, 28.1ms\n",
      "Speed: 1.1ms preprocess, 28.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.2ms\n",
      "Speed: 1.1ms preprocess, 22.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 30.6ms\n",
      "Speed: 1.2ms preprocess, 30.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 bottle, 1 chair, 29.6ms\n",
      "Speed: 1.2ms preprocess, 29.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 potted plant, 1 remote, 49.6ms\n",
      "Speed: 3.2ms preprocess, 49.6ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 30.1ms\n",
      "Speed: 1.3ms preprocess, 30.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.5ms\n",
      "Speed: 1.4ms preprocess, 27.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 31.3ms\n",
      "Speed: 1.3ms preprocess, 31.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 potted plant, 1 microwave, 24.9ms\n",
      "Speed: 1.1ms preprocess, 24.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 25.1ms\n",
      "Speed: 1.1ms preprocess, 25.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.0ms\n",
      "Speed: 1.7ms preprocess, 25.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 28.5ms\n",
      "Speed: 1.7ms preprocess, 28.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 23.9ms\n",
      "Speed: 1.1ms preprocess, 23.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 23.6ms\n",
      "Speed: 1.5ms preprocess, 23.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 29.6ms\n",
      "Speed: 1.8ms preprocess, 29.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 tie, 23.0ms\n",
      "Speed: 1.1ms preprocess, 23.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 31.8ms\n",
      "Speed: 4.0ms preprocess, 31.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 26.3ms\n",
      "Speed: 1.2ms preprocess, 26.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 chair, 1 couch, 1 tv, 25.8ms\n",
      "Speed: 1.2ms preprocess, 25.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 24.1ms\n",
      "Speed: 1.9ms preprocess, 24.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 27.5ms\n",
      "Speed: 1.1ms preprocess, 27.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 50.8ms\n",
      "Speed: 3.7ms preprocess, 50.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.1ms\n",
      "Speed: 1.4ms preprocess, 23.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 27.9ms\n",
      "Speed: 1.3ms preprocess, 27.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.9ms\n",
      "Speed: 1.1ms preprocess, 25.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 chairs, 32.3ms\n",
      "Speed: 2.8ms preprocess, 32.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 28.0ms\n",
      "Speed: 2.2ms preprocess, 28.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.4ms\n",
      "Speed: 1.1ms preprocess, 28.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 chair, 1 tv, 27.1ms\n",
      "Speed: 1.2ms preprocess, 27.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 tie, 23.8ms\n",
      "Speed: 1.1ms preprocess, 23.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 suitcase, 1 bottle, 29.9ms\n",
      "Speed: 1.2ms preprocess, 29.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 32.1ms\n",
      "Speed: 1.1ms preprocess, 32.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 potted plant, 31.8ms\n",
      "Speed: 1.1ms preprocess, 31.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.8ms\n",
      "Speed: 1.1ms preprocess, 26.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 33.2ms\n",
      "Speed: 1.4ms preprocess, 33.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.8ms\n",
      "Speed: 1.7ms preprocess, 24.8ms inference, 10.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 34.8ms\n",
      "Speed: 1.1ms preprocess, 34.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 24.2ms\n",
      "Speed: 1.1ms preprocess, 24.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 couch, 1 tv, 28.2ms\n",
      "Speed: 1.3ms preprocess, 28.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.7ms\n",
      "Speed: 1.3ms preprocess, 25.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.8ms\n",
      "Speed: 1.1ms preprocess, 24.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.4ms\n",
      "Speed: 1.2ms preprocess, 26.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.1ms\n",
      "Speed: 3.3ms preprocess, 26.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.0ms\n",
      "Speed: 1.4ms preprocess, 24.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.5ms\n",
      "Speed: 1.5ms preprocess, 24.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 30.4ms\n",
      "Speed: 1.2ms preprocess, 30.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 26.8ms\n",
      "Speed: 1.2ms preprocess, 26.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.0ms\n",
      "Speed: 1.1ms preprocess, 23.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.3ms\n",
      "Speed: 10.9ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.1ms\n",
      "Speed: 2.0ms preprocess, 25.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 26.6ms\n",
      "Speed: 1.1ms preprocess, 26.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 1.6ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 couch, 26.6ms\n",
      "Speed: 1.8ms preprocess, 26.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 49.3ms\n",
      "Speed: 8.8ms preprocess, 49.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 potted plant, 1 remote, 29.9ms\n",
      "Speed: 1.2ms preprocess, 29.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 45.0ms\n",
      "Speed: 14.9ms preprocess, 45.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.4ms\n",
      "Speed: 1.1ms preprocess, 27.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 1 potted plant, 24.8ms\n",
      "Speed: 1.8ms preprocess, 24.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.7ms\n",
      "Speed: 1.1ms preprocess, 28.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 spoon, 2 bananas, 1 orange, 30.7ms\n",
      "Speed: 1.2ms preprocess, 30.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 1 potted plant, 63.2ms\n",
      "Speed: 9.2ms preprocess, 63.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 fork, 2 bananas, 1 orange, 59.9ms\n",
      "Speed: 1.7ms preprocess, 59.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 24.3ms\n",
      "Speed: 1.5ms preprocess, 24.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 2 bananas, 1 orange, 41.8ms\n",
      "Speed: 1.8ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 potted plant, 26.7ms\n",
      "Speed: 1.2ms preprocess, 26.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 banana, 1 orange, 24.5ms\n",
      "Speed: 2.0ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 potted plant, 24.6ms\n",
      "Speed: 1.8ms preprocess, 24.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 banana, 1 orange, 1 cell phone, 49.3ms\n",
      "Speed: 1.3ms preprocess, 49.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 3 cakes, 4 chairs, 1 potted plant, 1 dining table, 2 vases, 34.6ms\n",
      "Speed: 1.9ms preprocess, 34.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 31.7ms\n",
      "Speed: 2.5ms preprocess, 31.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 26.5ms\n",
      "Speed: 1.1ms preprocess, 26.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 potted plant, 1 vase, 24.9ms\n",
      "Speed: 1.1ms preprocess, 24.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 wine glass, 1 spoon, 1 banana, 1 orange, 26.4ms\n",
      "Speed: 1.2ms preprocess, 26.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.4ms\n",
      "Speed: 1.4ms preprocess, 28.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 wine glass, 1 banana, 1 orange, 40.5ms\n",
      "Speed: 1.3ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 1 potted plant, 38.1ms\n",
      "Speed: 1.2ms preprocess, 38.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 1.9ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 25.3ms\n",
      "Speed: 1.2ms preprocess, 25.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.3ms\n",
      "Speed: 1.3ms preprocess, 25.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 1 refrigerator, 28.9ms\n",
      "Speed: 1.2ms preprocess, 28.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 51.5ms\n",
      "Speed: 6.6ms preprocess, 51.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 wine glass, 5 chairs, 1 potted plant, 1 dining table, 2 vases, 26.4ms\n",
      "Speed: 1.3ms preprocess, 26.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 potted plant, 26.9ms\n",
      "Speed: 1.2ms preprocess, 26.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.4ms\n",
      "Speed: 1.4ms preprocess, 27.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 potted plant, 24.1ms\n",
      "Speed: 1.3ms preprocess, 24.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tie, 26.1ms\n",
      "Speed: 1.3ms preprocess, 26.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 potted plant, 25.2ms\n",
      "Speed: 1.1ms preprocess, 25.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.3ms\n",
      "Speed: 1.1ms preprocess, 25.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 29.4ms\n",
      "Speed: 1.3ms preprocess, 29.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 ties, 27.3ms\n",
      "Speed: 1.5ms preprocess, 27.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.1ms\n",
      "Speed: 1.8ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 29.2ms\n",
      "Speed: 1.2ms preprocess, 29.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tie, 25.3ms\n",
      "Speed: 1.8ms preprocess, 25.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.8ms\n",
      "Speed: 1.6ms preprocess, 24.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.4ms\n",
      "Speed: 1.2ms preprocess, 28.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.1ms\n",
      "Speed: 1.1ms preprocess, 26.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.3ms\n",
      "Speed: 1.1ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.1ms\n",
      "Speed: 1.2ms preprocess, 26.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 32.0ms\n",
      "Speed: 1.2ms preprocess, 32.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.6ms\n",
      "Speed: 1.2ms preprocess, 26.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 47.9ms\n",
      "Speed: 1.3ms preprocess, 47.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.8ms\n",
      "Speed: 1.2ms preprocess, 24.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.9ms\n",
      "Speed: 1.2ms preprocess, 26.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 chairs, 30.7ms\n",
      "Speed: 3.1ms preprocess, 30.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tie, 27.0ms\n",
      "Speed: 1.3ms preprocess, 27.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 5 chairs, 25.8ms\n",
      "Speed: 1.2ms preprocess, 25.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 5 chairs, 26.9ms\n",
      "Speed: 1.2ms preprocess, 26.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 4 chairs, 27.5ms\n",
      "Speed: 1.2ms preprocess, 27.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.8ms\n",
      "Speed: 1.2ms preprocess, 26.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.1ms\n",
      "Speed: 1.1ms preprocess, 26.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.2ms\n",
      "Speed: 1.1ms preprocess, 27.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.7ms\n",
      "Speed: 1.3ms preprocess, 25.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 26.1ms\n",
      "Speed: 1.2ms preprocess, 26.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 4 chairs, 30.7ms\n",
      "Speed: 1.8ms preprocess, 30.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 1.7ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 4 chairs, 36.1ms\n",
      "Speed: 1.3ms preprocess, 36.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.1ms\n",
      "Speed: 1.6ms preprocess, 27.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.3ms\n",
      "Speed: 2.5ms preprocess, 28.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.2ms\n",
      "Speed: 1.1ms preprocess, 26.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.0ms\n",
      "Speed: 1.2ms preprocess, 26.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 book, 55.5ms\n",
      "Speed: 5.1ms preprocess, 55.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.3ms\n",
      "Speed: 1.2ms preprocess, 25.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.4ms\n",
      "Speed: 1.2ms preprocess, 24.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.6ms\n",
      "Speed: 1.2ms preprocess, 26.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.5ms\n",
      "Speed: 1.2ms preprocess, 28.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.2ms\n",
      "Speed: 1.2ms preprocess, 26.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.5ms\n",
      "Speed: 1.7ms preprocess, 25.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.8ms\n",
      "Speed: 1.1ms preprocess, 23.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 1.2ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 book, 23.6ms\n",
      "Speed: 1.1ms preprocess, 23.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.0ms\n",
      "Speed: 1.2ms preprocess, 27.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 32.3ms\n",
      "Speed: 1.5ms preprocess, 32.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 clock, 23.0ms\n",
      "Speed: 1.1ms preprocess, 23.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 33.2ms\n",
      "Speed: 1.3ms preprocess, 33.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 23.4ms\n",
      "Speed: 1.2ms preprocess, 23.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.4ms\n",
      "Speed: 1.3ms preprocess, 24.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.4ms\n",
      "Speed: 4.3ms preprocess, 28.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.6ms\n",
      "Speed: 1.3ms preprocess, 24.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.2ms\n",
      "Speed: 1.1ms preprocess, 25.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.8ms\n",
      "Speed: 1.8ms preprocess, 67.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 potted plant, 39.5ms\n",
      "Speed: 4.2ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.9ms\n",
      "Speed: 1.1ms preprocess, 25.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 couch, 24.9ms\n",
      "Speed: 1.2ms preprocess, 24.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 books, 25.0ms\n",
      "Speed: 1.2ms preprocess, 25.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "               Frame Object_ID    YOLO_Label Best_Match   Score\n",
      "0     keyframe_0.jpg         1  potted plant    A plant  0.9722\n",
      "1     keyframe_0.jpg         2  potted plant    A plant  0.9132\n",
      "2     keyframe_0.jpg         3  potted plant    A plant  0.9765\n",
      "3     keyframe_0.jpg         4  potted plant    A plant  0.9357\n",
      "4     keyframe_0.jpg         5  potted plant    A plant  0.9661\n",
      "..               ...       ...           ...        ...     ...\n",
      "524  keyframe_98.jpg         4         chair      A bed  0.2851\n",
      "525  keyframe_99.jpg         1        person    A Kurta  0.7755\n",
      "526  keyframe_99.jpg         2          book     A book  0.5913\n",
      "527  keyframe_99.jpg         3          book     A book  0.9909\n",
      "528  keyframe_99.jpg         4          book     A book  0.9303\n",
      "\n",
      "[529 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "import certifi\n",
    "import torch\n",
    "import open_clip\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "# Fix SSL issues\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "ssl._create_default_https_context = ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "# Load YOLOv8 model\n",
    "yolo_model = YOLO(\"yolov8n.pt\")  # Use \"yolov8m.pt\" or \"yolov8l.pt\" for better accuracy\n",
    "\n",
    "# Load OpenCLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess, _ = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "# Define text descriptions for classification\n",
    "text_descriptions = [\n",
    "    \"A Saree\",\n",
    "    \"A woman wearing a saree\",\n",
    "    \"A traditional Indian saree with embroidery\",\n",
    "    \"A long dress\",\n",
    "    \"t-shirt\",\n",
    "    \"pants\",\n",
    "    \"A Kurta\",\n",
    "    \"A t-shirt\",\n",
    "    \"A Necklace\",\n",
    "    \"A bag\",\n",
    "    \"A chair\",\n",
    "    \"A car\",\n",
    "    \"A plant\",\n",
    "    \"Spectacles\",\n",
    "    \"A pair of shoes\",\n",
    "    \"A lamp\",\n",
    "    \"A watch\",\n",
    "    \"A spoon\",\n",
    "    \"Flowers\",\n",
    "    \"A bed\",\n",
    "    \"A rangoli\",\n",
    "    \"A book\",\n",
    "    \"A ring\"\n",
    "]\n",
    "\n",
    "# Tokenize text descriptions for CLIP\n",
    "text_inputs = open_clip.tokenize(text_descriptions).to(device)\n",
    "\n",
    "# Define the folder containing frames\n",
    "frame_folder = \"/Users/aarushia/Downloads/shows/frames/kyuki saas/ksbkbt_s1e1_thres30\"  # Update with the correct path\n",
    "\n",
    "# Ensure the frame folder exists\n",
    "if not os.path.exists(frame_folder):\n",
    "    raise FileNotFoundError(f\"Frame folder not found: {frame_folder}\")\n",
    "\n",
    "# Get all frame images\n",
    "frame_files = sorted([f for f in os.listdir(frame_folder) if f.endswith((\".jpg\", \".png\"))])\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "df_results = pd.DataFrame(columns=[\"Frame\", \"Object_ID\", \"YOLO_Label\", \"Best_Match\", \"Score\"])\n",
    "\n",
    "# Loop through all frames\n",
    "for frame_id, frame_file in enumerate(frame_files):\n",
    "    frame_path = os.path.join(frame_folder, frame_file)\n",
    "    image = cv2.imread(frame_path)\n",
    "    if image is None:\n",
    "        continue  # Skip if the image cannot be loaded\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for PIL\n",
    "\n",
    "    # Run YOLOv8 detection on the frame\n",
    "    results = yolo_model(image)\n",
    "\n",
    "    # Process each detected object\n",
    "    object_count = 0\n",
    "    for result in results:\n",
    "        for box, cls in zip(result.boxes.xyxy, result.boxes.cls):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "            # Get YOLO's class label\n",
    "            yolo_label = yolo_model.names[int(cls)]  # Retrieve object name\n",
    "\n",
    "            # Crop the detected object\n",
    "            cropped_image = image_rgb[y1:y2, x1:x2]\n",
    "            if cropped_image.shape[0] == 0 or cropped_image.shape[1] == 0:\n",
    "                continue  # Skip if cropping resulted in an empty image\n",
    "\n",
    "            cropped_pil = Image.fromarray(cropped_image)  # Convert to PIL format\n",
    "\n",
    "            # Preprocess cropped image for CLIP\n",
    "            image_tensor = preprocess(cropped_pil).unsqueeze(0).to(device)\n",
    "\n",
    "            # Compute CLIP image and text embeddings\n",
    "            with torch.no_grad():\n",
    "                image_features = model.encode_image(image_tensor)\n",
    "                text_features = model.encode_text(text_inputs)\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            similarity = (image_features @ text_features.T).softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "            # Get best matching label\n",
    "            best_match_idx = similarity.argmax()\n",
    "            best_match = text_descriptions[best_match_idx]\n",
    "            best_match_score = similarity[0][best_match_idx]\n",
    "\n",
    "            # Append to DataFrame\n",
    "            df_results = pd.concat([df_results, pd.DataFrame([{\n",
    "                \"Frame\": frame_file,\n",
    "                \"Object_ID\": object_count + 1,\n",
    "                \"YOLO_Label\": yolo_label,\n",
    "                \"Best_Match\": best_match,\n",
    "                \"Score\": round(best_match_score, 4)\n",
    "            }])], ignore_index=True)\n",
    "\n",
    "            object_count += 1  # Increment object count in the frame\n",
    "\n",
    "# Save results to CSV\n",
    "df_results.to_csv(\"detected_objects_clip_1.csv\", index=False)\n",
    "\n",
    "# Display the results\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aarushia/Library/Python/3.13/lib/python/site-packages/open_clip/factory.py:380: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "gpt-3.5-turbo is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/gpt-3.5-turbo/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/huggingface_hub/file_download.py:967\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/huggingface_hub/file_download.py:1482\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/huggingface_hub/file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/huggingface_hub/file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/huggingface_hub/file_download.py:302\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 302\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-67aaf5d3-2a0d84e86333fcba2c8eb078;4b0b8f7f-2c5e-4a13-a361-9916c0ce9393)\n\nRepository Not Found for url: https://huggingface.co/gpt-3.5-turbo/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOLO_Label\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest_Match\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Load GPT model for text generation (to create dynamic descriptions)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Loop through all frames\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_id, frame_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(frame_files):\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/transformers/pipelines/__init__.py:812\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m     pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig) \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/transformers/utils/hub.py:426\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: gpt-3.5-turbo is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "# uses gpt to generate text descriptions for detected objects in frames CHECK THIS HAVENT RUN YET\n",
    "import ssl\n",
    "import certifi\n",
    "import torch\n",
    "import open_clip\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "\n",
    "# Fix SSL issues\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "ssl._create_default_https_context = ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "# Load YOLOv8 model\n",
    "yolo_model = YOLO(\"yolov8n.pt\")  # Use \"yolov8m.pt\" or \"yolov8l.pt\" for better accuracy\n",
    "\n",
    "# Load OpenCLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess, _ = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "# Define the folder containing frames\n",
    "frame_folder = \"/Users/aarushia/Downloads/shows/frames/kyuki saas/ksbkbt_s1e1_thres30\"  # Update with the correct path\n",
    "\n",
    "# Ensure the frame folder exists\n",
    "if not os.path.exists(frame_folder):\n",
    "    raise FileNotFoundError(f\"Frame folder not found: {frame_folder}\")\n",
    "\n",
    "# Get all frame images\n",
    "frame_files = sorted([f for f in os.listdir(frame_folder) if f.endswith((\".jpg\", \".png\"))])\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "df_results = pd.DataFrame(columns=[\"Frame\", \"Object_ID\", \"YOLO_Label\", \"Best_Match\", \"Score\"])\n",
    "\n",
    "# Load GPT model for text generation (to create dynamic descriptions)\n",
    "generator = pipeline(\"text-generation\", model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Loop through all frames\n",
    "for frame_id, frame_file in enumerate(frame_files):\n",
    "    frame_path = os.path.join(frame_folder, frame_file)\n",
    "    image = cv2.imread(frame_path)\n",
    "    if image is None:\n",
    "        continue  # Skip if the image cannot be loaded\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for PIL\n",
    "\n",
    "    # Run YOLOv8 detection on the frame\n",
    "    results = yolo_model(image)\n",
    "\n",
    "    # Process each detected object\n",
    "    object_count = 0\n",
    "    for result in results:\n",
    "        for box, cls in zip(result.boxes.xyxy, result.boxes.cls):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "            # Get YOLO's class label\n",
    "            yolo_label = yolo_model.names[int(cls)]\n",
    "\n",
    "            # Crop the detected object\n",
    "            cropped_image = image_rgb[y1:y2, x1:x2]\n",
    "            if cropped_image.shape[0] == 0 or cropped_image.shape[1] == 0:\n",
    "                continue  # Skip if cropping resulted in an empty image\n",
    "\n",
    "            cropped_pil = Image.fromarray(cropped_image)  # Convert to PIL format\n",
    "\n",
    "            # Generate text descriptions dynamically using GPT\n",
    "            description_prompt = f\"Describe an object that looks like {yolo_label} in an image.\"\n",
    "            text_descriptions = generator(description_prompt, max_length=50, num_return_sequences=5)\n",
    "            text_descriptions = [t['generated_text'] for t in text_descriptions]\n",
    "\n",
    "            # Preprocess cropped image for CLIP\n",
    "            image_tensor = preprocess(cropped_pil).unsqueeze(0).to(device)\n",
    "\n",
    "            # Tokenize text descriptions\n",
    "            text_inputs = open_clip.tokenize(text_descriptions).to(device)\n",
    "\n",
    "            # Compute CLIP image and text embeddings\n",
    "            with torch.no_grad():\n",
    "                image_features = model.encode_image(image_tensor)\n",
    "                text_features = model.encode_text(text_inputs)\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            similarity = (image_features @ text_features.T).softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "            # Get best matching label\n",
    "            best_match_idx = similarity.argmax()\n",
    "            best_match = text_descriptions[best_match_idx]\n",
    "            best_match_score = similarity[0][best_match_idx]\n",
    "\n",
    "            # Append to DataFrame\n",
    "            df_results = pd.concat([df_results, pd.DataFrame([{\n",
    "                \"Frame\": frame_file,\n",
    "                \"Object_ID\": object_count + 1,\n",
    "                \"YOLO_Label\": yolo_label,\n",
    "                \"Best_Match\": best_match,\n",
    "                \"Score\": round(best_match_score, 4)\n",
    "            }])], ignore_index=True)\n",
    "\n",
    "            object_count += 1  # Increment object count in the frame\n",
    "\n",
    "# Save results to CSV\n",
    "df_results.to_csv(\"detected_objects_clip.csv\", index=False)\n",
    "\n",
    "# Display the results\n",
    "print(df_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
